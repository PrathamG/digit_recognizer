{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Digit Recognizer\nThe MNIST dataset is a collecction of 28x28 pixel images of handwritten single-digit numbers. We will develop a deep neural network that classifies these images from 0 to 9. \n\nWe begin by:\n* Loading the necessary modules\n* Loading and preprocessing the training data, label data and test data into separate variables"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\nfrom keras.utils import np_utils\n\ntrain_data = np.genfromtxt('../input/digit-recognizer/train.csv', delimiter=',', skip_header = 1)\nY = train_data[:,:1]\nY = Y.reshape((Y.shape[0], 1))\nY = np_utils.to_categorical(Y)\nX = train_data[:,1:]\nX = X.reshape((X.shape[0], 28, 28, 1))\nX = X/255\n\ntest_data = np.genfromtxt('../input/digit-recognizer/test.csv', delimiter=',', skip_header = 1)\nX_test = test_data[:,:]\nX_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\nX_test = X_test/255\n\nY[0:2]","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The below code cell allows us to observe the images in the training data. As the example below shows, the 4th image (index 3) in the dataset is a handwritten character 4, labeled likewise."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimg_num = 3\nplt.imshow(X[img_num].reshape(28,28), cmap='gray',interpolation='none')\nprint(train_data[img_num][0])","execution_count":44,"outputs":[{"output_type":"stream","text":"4.0\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"# Developing the model\n\nWe now develop a function that fits a training model to the training data and returns the corresponding statistics such as loss and validation accuracy at each epoch of model training. The function runs a simple model that contains 2 convolutional layers and 1 fully connected layer, each with ReLU activation. These are followed by 1 fully connected layer of Softmax activation which gives us the final result. Max pooling is done between the convolutional layers and Batch normalization is done at every step.\n\nThe purpose of this function is to test different hyperparameters to develop the most accurate model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_model(cat_size, num_filter_1, num_filter_2, ksize_1, ksize_2, dense_size):\n    base_model = Sequential()\n    base_model.add(Conv2D(num_filter_1, \n                          kernel_size = ksize_1,\n                          strides = 1,\n                          activation = 'relu',\n                          input_shape = (28,28,1))\n                  )\n    base_model.add(MaxPooling2D(pool_size=(2, 2), padding = 'valid'))\n    base_model.add(BatchNormalization())\n    base_model.add(Conv2D(num_filter_2, \n                          kernel_size = ksize_2,\n                          strides = 1,\n                          activation = 'relu',)\n                  )\n    base_model.add(BatchNormalization())\n    base_model.add(Flatten())\n    base_model.add(Dense(dense_size, activation = \"relu\"))\n    base_model.add(BatchNormalization())\n    base_model.add(Dense(cat_size, activation = 'softmax'))\n\n    base_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\n    result = (base_model.fit(X, Y, batch_size=32, epochs=15, validation_split = 0.2, verbose = 0))\n    return result","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tuning the filter sizes for the 2 convolutional networks:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_size = 10\nnum_filter_1 = 16\nnum_filter_2 = 32\nksize_1 = 3\nksize_2 = 5\ndense_size = 32\n\nksize_list = [(3, 3), (3, 5), (5, 5)]\nfor ksize_1, ksize_2 in ksize_list:\n    result = run_model(cat_size, num_filter_1, num_filter_2, ksize_1, ksize_2, dense_size)\n    print(\"Filter1: \" + str(ksize_1) + \" Filter2: \" + str(ksize_2) + \" Val_acc: \" + str(max(result.history['val_accuracy'])))","execution_count":24,"outputs":[{"output_type":"stream","text":"Filter1: 3 Filter2: 3 Val_acc: 0.989047646522522\nFilter1: 3 Filter2: 5 Val_acc: 0.9901190400123596\nFilter1: 5 Filter2: 5 Val_acc: 0.9903571605682373\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Tuning the number of channels in the 2 convolutional layers:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ksize_1 = 5\nksize_2 = 5\nnum_filter_list = [(8,16), (16,32), (32,64), (64,128)]\nfor num_filter_1, num_filter_2 in num_filter_list:\n    result = run_model(cat_size, num_filter_1, num_filter_2, ksize_1, ksize_2, dense_size)\n    print(\"Channel1: \" + str(num_filter_1) + \" Channel2: \" + str(num_filter_2) + \" Val_acc: \" + str(max(result.history['val_accuracy'])))","execution_count":26,"outputs":[{"output_type":"stream","text":"Channel1: 8 Channel2: 16 Val_acc: 0.9900000095367432\nChannel1: 16 Channel2: 32 Val_acc: 0.991190493106842\nChannel1: 32 Channel2: 64 Val_acc: 0.9916666746139526\nChannel1: 64 Channel2: 128 Val_acc: 0.9915476441383362\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Tuning the number of nodes in the ReLU fully connected layer:"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_filter_1 = 32\nnum_filter_2 = 64\ndense_list = [16, 32, 64, 128, 256, 512, 1024]\nfor dense_size in dense_list:\n    result = run_model(cat_size, num_filter_1, num_filter_2, ksize_1, ksize_2, dense_size)\n    print(\"Dense_size: \" + str(dense_size) + \" Val_acc: \" + str(max(result.history['val_accuracy'])))","execution_count":27,"outputs":[{"output_type":"stream","text":"Dense_size: 16 Val_acc: 0.991190493106842\nDense_size: 32 Val_acc: 0.9922618865966797\nDense_size: 64 Val_acc: 0.9920238256454468\nDense_size: 128 Val_acc: 0.9921428561210632\nDense_size: 256 Val_acc: 0.9915476441383362\nDense_size: 512 Val_acc: 0.9920238256454468\nDense_size: 1024 Val_acc: 0.9905952215194702\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Training the model\n\nWhen choosing the best hyperparameters above, along with model accuracy, we also give consideration to model training time. Hence, in some cases, we chose hyperparameters that were not the best in accuracy. The increase in accuracy was not significant enough to warrant the increase in computational load. \n\nWe now fit the model to the training data with the most effective hyperparameters we found. "},{"metadata":{"trusted":true},"cell_type":"code","source":"ksize_1 = 5\nksize_2 = 5\nnum_filter_1 = 32\nnum_filter_2 = 64\ndense_size = 32\n\nself_model = Sequential()\nself_model.add(Conv2D(num_filter_1, \n                      kernel_size = ksize_1,\n                      strides = 1,\n                      activation = 'relu',\n                      input_shape = (28,28,1))\n              )\nself_model.add(MaxPooling2D(pool_size=(2, 2), padding = 'valid'))\nself_model.add(BatchNormalization())\nself_model.add(Conv2D(num_filter_2, \n                      kernel_size = ksize_2,\n                      strides = 1,\n                      activation = 'relu',)\n              )\nself_model.add(BatchNormalization())\nself_model.add(Flatten())\nself_model.add(Dense(dense_size, activation = \"relu\"))\nself_model.add(BatchNormalization())\nself_model.add(Dense(cat_size, activation = 'softmax'))\n\nself_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\nself_model.fit(X, Y, batch_size=32, epochs=15, validation_split = 0.2)","execution_count":29,"outputs":[{"output_type":"stream","text":"Epoch 1/15\n1050/1050 [==============================] - 4s 4ms/step - loss: 0.1322 - accuracy: 0.9651 - val_loss: 0.0544 - val_accuracy: 0.9839\nEpoch 2/15\n1050/1050 [==============================] - 4s 4ms/step - loss: 0.0458 - accuracy: 0.9864 - val_loss: 0.0469 - val_accuracy: 0.9861\nEpoch 3/15\n1050/1050 [==============================] - 5s 5ms/step - loss: 0.0305 - accuracy: 0.9909 - val_loss: 0.0387 - val_accuracy: 0.9877\nEpoch 4/15\n1050/1050 [==============================] - 4s 4ms/step - loss: 0.0250 - accuracy: 0.9926 - val_loss: 0.0412 - val_accuracy: 0.9890\nEpoch 5/15\n1050/1050 [==============================] - 5s 5ms/step - loss: 0.0194 - accuracy: 0.9939 - val_loss: 0.0630 - val_accuracy: 0.9825\nEpoch 6/15\n1050/1050 [==============================] - 4s 4ms/step - loss: 0.0173 - accuracy: 0.9950 - val_loss: 0.0362 - val_accuracy: 0.9906\nEpoch 7/15\n1050/1050 [==============================] - 4s 4ms/step - loss: 0.0110 - accuracy: 0.9965 - val_loss: 0.0598 - val_accuracy: 0.9844\nEpoch 8/15\n1050/1050 [==============================] - 4s 4ms/step - loss: 0.0112 - accuracy: 0.9968 - val_loss: 0.0416 - val_accuracy: 0.9907\nEpoch 9/15\n1050/1050 [==============================] - 4s 4ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 0.0503 - val_accuracy: 0.9869\nEpoch 10/15\n1050/1050 [==============================] - 4s 4ms/step - loss: 0.0099 - accuracy: 0.9972 - val_loss: 0.0645 - val_accuracy: 0.9825\nEpoch 11/15\n1050/1050 [==============================] - 4s 4ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0343 - val_accuracy: 0.9915\nEpoch 12/15\n1050/1050 [==============================] - 4s 4ms/step - loss: 0.0069 - accuracy: 0.9981 - val_loss: 0.0367 - val_accuracy: 0.9915\nEpoch 13/15\n1050/1050 [==============================] - 4s 4ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.0414 - val_accuracy: 0.9896\nEpoch 14/15\n1050/1050 [==============================] - 4s 4ms/step - loss: 0.0043 - accuracy: 0.9984 - val_loss: 0.0337 - val_accuracy: 0.9930\nEpoch 15/15\n1050/1050 [==============================] - 4s 4ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.0352 - val_accuracy: 0.9925\n","name":"stdout"},{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f9516474dd0>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The model is estimated to give an accuracy of about 0.99 on the test data. We run the model on the test data and save the results to a CSV file for submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = self_model.predict_classes(X_test)\nimg_id = np.array(range(0, X_test.shape[0]))\nimg_id += 1\npreds_id = np.vstack((img_id, preds)).T.astype(int)\npreds_pd = pd.DataFrame(preds_id, columns = ['ImageId', 'Label'])\npreds_pd.set_index('ImageId', inplace = True)\npreds_pd.to_csv('output_base.csv')","execution_count":30,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using a pretrained model\n\nWe now use the pretrained ResNet50 model and adjust it to solve our problem. We then compare the performance of our custom model with that of the ResNet50 architecture. \n\n* We adjust our input data by padding it with 2 layers to meet the minimum 32 x 32 pixel size needed for ResNet50. We then duplicate every image into 3 channels as this architecture only works on 3 channel images.\n* We remove the final layer of the ResNet50 model, and replace it with 2 dense ReLU layers followed by one Softmax layer with 10 classes.\n* We then train the entire model on our training data. Since it already contains the pretrained weights, the training time should not be very high."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications import ResNet50\n\nnum_classes = 10\n\nnpad = ((0, 0), (2, 2), (2, 2), (0, 0))\nX_pad = np.pad(X, pad_width=npad, mode='constant', constant_values=0)\nX_mnet = np.repeat(X_pad, 3, -1)\n\nmodel = Sequential()\nmodel.add(ResNet50(include_top = False, input_shape = (32,32,3)))\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation = 'relu'))\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dense(num_classes, activation = 'softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(X_mnet, Y, batch_size = 64, epochs = 7, validation_split = 0.2)\n","execution_count":33,"outputs":[{"output_type":"stream","text":"Epoch 1/7\n525/525 [==============================] - 24s 46ms/step - loss: 0.3227 - accuracy: 0.9269 - val_loss: 2.5242 - val_accuracy: 0.2395\nEpoch 2/7\n525/525 [==============================] - 23s 44ms/step - loss: 0.2050 - accuracy: 0.9591 - val_loss: 0.2209 - val_accuracy: 0.9368\nEpoch 3/7\n525/525 [==============================] - 23s 43ms/step - loss: 0.0788 - accuracy: 0.9788 - val_loss: 0.0585 - val_accuracy: 0.9838\nEpoch 4/7\n525/525 [==============================] - 23s 44ms/step - loss: 0.0748 - accuracy: 0.9830 - val_loss: 0.0798 - val_accuracy: 0.9796\nEpoch 5/7\n525/525 [==============================] - 23s 43ms/step - loss: 0.0476 - accuracy: 0.9870 - val_loss: 0.0563 - val_accuracy: 0.9867\nEpoch 6/7\n525/525 [==============================] - 23s 44ms/step - loss: 0.0484 - accuracy: 0.9873 - val_loss: 0.1249 - val_accuracy: 0.9706\nEpoch 7/7\n525/525 [==============================] - 23s 44ms/step - loss: 0.0551 - accuracy: 0.9858 - val_loss: 0.0598 - val_accuracy: 0.9846\n","name":"stdout"},{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f99d9d2ed90>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"npad = ((0, 0), (2, 2), (2, 2), (0, 0))\nX_test_pad = np.pad(X_test, pad_width=npad, mode='constant', constant_values=0)\nX_test_rnet = np.repeat(X_test_pad, 3, -1)\npreds = model.predict_classes(X_test_rnet)\n\nimg_id = np.array(range(0, X_test.shape[0]))\nimg_id += 1\npreds_id = np.vstack((img_id, preds)).T.astype(int)\npreds_id_pd = pd.DataFrame(preds_id, columns = ['ImageId', 'Label'])\npreds_id_pd.set_index('ImageId', inplace = True)\npreds_id_pd.to_csv('output_rn50.csv')","execution_count":34,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Results\n\nThis model can be estimated to give an accuracy of 0.97-0.98 on the test data. In fact, when the results were submitted on Kaggle:\n* The custom model had a test accuracy of 0.99075\n* The ResNet50 model had a test accuracy of 0.98585.\n\nA possible explanation for the way more advanced ResNet50 architecture to work more poorly on the training data could be that our original inupt data of 28x28x1 images contains too less data for this 50 layer deep model to train effectively on. The ResNet was built for RGB images of size 224x224, hence the model may have overfit our data."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}